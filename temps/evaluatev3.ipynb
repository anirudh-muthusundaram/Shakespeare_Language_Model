{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_2672\\3833604820.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"shakespeare_generator.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1034, Perplexity: 1.1089, Accuracy: 96.72%\n",
      "Sample generated text:\n",
      " Romeo and Juliet.\n",
      "\n",
      "\n",
      "_Luc._ She never reprehended him but mildly,\n",
      "When he demean'd himself rough, rude, and wildly.\n",
      "Why bear you these rebukes, and answer not?\n",
      "\n",
      "_Adr._ She did betray me to my own reproof.                   245\n",
      "They fell upon me, bound me, bore me thence,\n",
      "And in a dark and dankish vault at home\n",
      "There left me in.\n",
      "\n",
      "_Dro. S._ [_Within_] Nor to-day here you must not; come again\n",
      "        when you may.\n",
      "\n",
      "_Ant. E._ What art thou that keepest them.\n",
      "\n",
      "_Adr._ But say, I prithee, is he coming home?            \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Re-create the same preprocessing steps as training\n",
    "OUTPUT_DIR = \"shakespeare_works\"\n",
    "combined_text_file = \"data_shakespeare.txt\"\n",
    "\n",
    "with open(combined_text_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "# Use the same parameters as training\n",
    "vocab_size = len(chars)\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the same model\n",
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super(ShakespeareModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, rnn_units, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(rnn_units, rnn_units, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_units, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = ShakespeareModel(vocab_size, embedding_dim, rnn_units).to(device)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load(\"shakespeare_generator.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "text_as_int = np.array([char_to_idx[c] for c in text])\n",
    "\n",
    "seq_length = 100  # Example, ensure this matches your training seq_length\n",
    "split_ratio = 0.9\n",
    "split_index = int(len(text_as_int) * split_ratio)\n",
    "\n",
    "train_data = text_as_int[:split_index]\n",
    "val_data = text_as_int[split_index:]\n",
    "\n",
    "if len(val_data) <= seq_length:\n",
    "    raise ValueError(\"Validation data is too small. Increase your dataset or reduce seq_length.\")\n",
    "\n",
    "val_inputs = []\n",
    "val_targets = []\n",
    "examples_per_epoch_val = len(val_data) - seq_length\n",
    "for i in range(examples_per_epoch_val):\n",
    "    val_inputs.append(val_data[i:i+seq_length])\n",
    "    val_targets.append(val_data[i+1:i+1+seq_length])\n",
    "\n",
    "val_inputs = np.array(val_inputs)\n",
    "val_targets = np.array(val_targets)\n",
    "\n",
    "# Step 5: Create Validation DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "val_dataset = ShakespeareDataset(val_inputs, val_targets)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "            outputs = model(batch_inputs)  # [batch, seq_length, vocab_size]\n",
    "\n",
    "            outputs_flat = outputs.view(-1, vocab_size)\n",
    "            targets_flat = batch_targets.view(-1)\n",
    "            loss = criterion(outputs_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "            # Compute character-level accuracy\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            correct = (predictions == batch_targets).sum().item()\n",
    "            total_correct += correct\n",
    "            total_tokens += targets_flat.numel()\n",
    "\n",
    "    avg_loss = total_loss / count\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    accuracy = (total_correct / total_tokens) * 100\n",
    "    return avg_loss, perplexity, accuracy\n",
    "\n",
    "val_loss, val_perplexity, val_accuracy = evaluate_model(model, val_loader)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Perplexity: {val_perplexity:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "def generate_text(model, start_string, num_generate=500, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_eval = torch.tensor([char_to_idx[char] for char in start_string], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    text_generated = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_generate):\n",
    "            outputs = model(input_eval)  # [1, seq_length, vocab_size]\n",
    "            predictions = outputs[:, -1, :] / temperature\n",
    "            probs = torch.softmax(predictions, dim=-1)\n",
    "            predicted_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            text_generated.append(idx_to_char[predicted_id])\n",
    "            input_eval = torch.cat([input_eval[:, 1:], torch.tensor([[predicted_id]], device=device)], dim=1)\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "seed_text = \"Romeo and Juliet\"\n",
    "sample = generate_text(model, seed_text, num_generate=500, temperature=1.0)\n",
    "print(\"Sample generated text:\\n\", sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
